<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00930"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.02627">
<title>Inferencing Based on Unsupervised Learning of Disentangled Representations. (arXiv:1803.02627v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.02627</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining Generative Adversarial Networks (GANs) with encoders that learn to
encode data points has shown promising results in learning data representations
in an unsupervised way. We propose a framework that combines an encoder and a
generator to learn disentangled representations which encode meaningful
information about the data distribution without the need for any labels. While
current approaches focus mostly on the generative aspects of GANs, our
framework can be used to perform inference on both real and generated data
points. Experiments on several data sets show that the encoder learns
interpretable, disentangled representations which encode descriptive properties
and can be used to sample images that exhibit specific characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinz_T/0/1/0/all/0/1&quot;&gt;Tobias Hinz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00930">
<title>Beyond black-boxes in Bayesian inverse problems and model validation: applications in solid mechanics of elastography. (arXiv:1803.00930v3 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00930</link>
<description rdf:parseType="Literal">&lt;p&gt;The present paper is motivated by one of the most fundamental challenges in
inverse problems, that of quantifying model discrepancies and errors. While
significant strides have been made in calibrating model parameters, the
overwhelming majority of pertinent methods is based on the assumption of a
perfect model. Motivated by problems in solid mechanics which, as all problems
in continuum thermodynamics, are described by conservation laws and
phenomenological constitutive closures, we argue that in order to quantify
model uncertainty in a physically meaningful manner, one should break open the
black-box forward model. In particular we propose formulating an undirected
probabilistic model that explicitly accounts for the governing equations and
their validity. This recasts the solution of both forward and inverse problems
as probabilistic inference tasks where the problem&apos;s state variables should not
only be compatible with the data but also with the governing equations as well.
Even though the probability densities involved do not contain any black-box
terms, they live in much higher-dimensional spaces. In combination with the
intractability of the normalization constant of the undirected model employed,
this poses significant challenges which we propose to address with a
linearly-scaling, double-layer of Stochastic Variational Inference. We
demonstrate the capabilities and efficacy of the proposed model in synthetic
forward and inverse problems (with and without model error) in elastography.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bruder_L/0/1/0/all/0/1&quot;&gt;Lukas Bruder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Koutsourelakis_P/0/1/0/all/0/1&quot;&gt;Phaedon-Stelios Koutsourelakis&lt;/a&gt;</dc:creator>
</item></rdf:RDF>